{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flappy_bird.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WyuImk494j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca9fe78-18d9-49bd-a587-ab4174128aa9"
      },
      "source": [
        "! pip install pygame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/9e/c400554dd1d0e562bd4379f35ad5023c68fc120003a58991405850f56f95/pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 21.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTtBd07b5J-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19f134a-98ae-4773-f583-be5276202ea3"
      },
      "source": [
        "import pygame\n",
        "from pygame.locals import *\n",
        "from itertools import cycle\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import sys\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER'] = 'dummy' # Run Headless Pygame environment"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Psj4Sk7UBV"
      },
      "source": [
        "! mkdir /content/assets\n",
        "! mkdir /content/assets/sprites\n",
        "! mv *.png /content/assets/sprites\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iARa-ooG8d7y"
      },
      "source": [
        "## Load Game Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW7fLQIJ8lKE"
      },
      "source": [
        "def getHitmask(image):\n",
        "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
        "    mask = []\n",
        "    for x in range(image.get_width()):\n",
        "        mask.append([])\n",
        "        for y in range(image.get_height()):\n",
        "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
        "    return mask\n",
        "\n",
        "def load():\n",
        "    # path of player with different states\n",
        "    PLAYER_PATH = (\n",
        "            '/content/assets/sprites/redbird-upflap.png',\n",
        "            '/content/assets/sprites/redbird-midflap.png',\n",
        "            '/content/assets/sprites/redbird-downflap.png'\n",
        "    )\n",
        "\n",
        "    # path of background\n",
        "    BACKGROUND_PATH = '/content/assets/sprites/background-black.png'\n",
        "\n",
        "    # path of pipe\n",
        "    PIPE_PATH = '/content/assets/sprites/pipe-green.png'\n",
        "\n",
        "    IMAGES, HITMASKS = {}, {}\n",
        "\n",
        "    # numbers sprites for score display\n",
        "    IMAGES['numbers'] = (\n",
        "        pygame.image.load('/content/assets/sprites/0.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/1.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/2.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/3.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/4.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/5.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/6.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/7.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/8.png').convert_alpha(),\n",
        "        pygame.image.load('/content/assets/sprites/9.png').convert_alpha()\n",
        "    )\n",
        "\n",
        "    # base (ground) sprite\n",
        "    IMAGES['base'] = pygame.image.load('/content/assets/sprites/base.png').convert_alpha()\n",
        "\n",
        "    # select random background sprites\n",
        "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
        "\n",
        "    # select random player sprites\n",
        "    IMAGES['player'] = (\n",
        "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
        "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
        "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
        "    )\n",
        "\n",
        "    # select random pipe sprites\n",
        "    IMAGES['pipe'] = (\n",
        "        pygame.transform.rotate(\n",
        "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
        "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
        "    )\n",
        "\n",
        "    # hismask for pipes\n",
        "    HITMASKS['pipe'] = (\n",
        "        getHitmask(IMAGES['pipe'][0]),\n",
        "        getHitmask(IMAGES['pipe'][1]),\n",
        "    )\n",
        "\n",
        "    # hitmask for player\n",
        "    HITMASKS['player'] = (\n",
        "        getHitmask(IMAGES['player'][0]),\n",
        "        getHitmask(IMAGES['player'][1]),\n",
        "        getHitmask(IMAGES['player'][2]),\n",
        "    )\n",
        "    return IMAGES, HITMASKS"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkDXaaaG5w6f"
      },
      "source": [
        "## Game Parameters Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPItoR10564x"
      },
      "source": [
        "FPS = 30\n",
        "SCREENWIDTH = 288\n",
        "SCREENHEIGHT = 512\n",
        "\n",
        "pygame.init()\n",
        "FPSCLOCK = pygame.time.Clock()\n",
        "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
        "pygame.display.set_caption('Flappy Bird')\n",
        "\n",
        "IMAGES, HITMASKS = load()\n",
        "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
        "BASEY = SCREENHEIGHT * 0.79\n",
        "\n",
        "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
        "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
        "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
        "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
        "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
        "\n",
        "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
        "\n",
        "\n",
        "class GameState:\n",
        "    def __init__(self):\n",
        "        self.score = self.playerIndex = self.loopIter = 0\n",
        "        self.playerx = int(SCREENWIDTH * 0.2)\n",
        "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
        "        self.basex = 0\n",
        "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
        "\n",
        "        newPipe1 = getRandomPipe()\n",
        "        newPipe2 = getRandomPipe()\n",
        "        self.upperPipes = [\n",
        "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
        "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
        "        ]\n",
        "        self.lowerPipes = [\n",
        "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
        "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
        "        ]\n",
        "\n",
        "        # player velocity, max velocity, downward accleration, accleration on flap\n",
        "        self.pipeVelX = -4\n",
        "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
        "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
        "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
        "        self.playerAccY    =   1   # players downward accleration\n",
        "        self.playerFlapAcc =  -9   # players speed on flapping\n",
        "        self.playerFlapped = False # True when player flaps\n",
        "\n",
        "    def frame_step(self, input_actions):\n",
        "        pygame.event.pump()\n",
        "\n",
        "        reward = 0.1\n",
        "        terminal = False\n",
        "\n",
        "        if sum(input_actions) != 1:\n",
        "            raise ValueError('Multiple input actions!')\n",
        "\n",
        "        # input_actions[0] == 1: do nothing\n",
        "        # input_actions[1] == 1: flap the bird\n",
        "        if input_actions[1] == 1:\n",
        "            if self.playery > -2 * PLAYER_HEIGHT:\n",
        "                self.playerVelY = self.playerFlapAcc\n",
        "                self.playerFlapped = True\n",
        "\n",
        "        # check for score\n",
        "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
        "        for pipe in self.upperPipes:\n",
        "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
        "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
        "                self.score += 1\n",
        "                reward = 1\n",
        "\n",
        "        # playerIndex basex change\n",
        "        if (self.loopIter + 1) % 3 == 0:\n",
        "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
        "        self.loopIter = (self.loopIter + 1) % 30\n",
        "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
        "\n",
        "        # player's movement\n",
        "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
        "            self.playerVelY += self.playerAccY\n",
        "        if self.playerFlapped:\n",
        "            self.playerFlapped = False\n",
        "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
        "        if self.playery < 0:\n",
        "            self.playery = 0\n",
        "\n",
        "        # move pipes to left\n",
        "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
        "            uPipe['x'] += self.pipeVelX\n",
        "            lPipe['x'] += self.pipeVelX\n",
        "\n",
        "        # add new pipe when first pipe is about to touch left of screen\n",
        "        if 0 < self.upperPipes[0]['x'] < 5:\n",
        "            newPipe = getRandomPipe()\n",
        "            self.upperPipes.append(newPipe[0])\n",
        "            self.lowerPipes.append(newPipe[1])\n",
        "\n",
        "        # remove first pipe if its out of the screen\n",
        "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
        "            self.upperPipes.pop(0)\n",
        "            self.lowerPipes.pop(0)\n",
        "\n",
        "        # check if crash here\n",
        "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
        "                             'index': self.playerIndex},\n",
        "                            self.upperPipes, self.lowerPipes)\n",
        "        if isCrash:\n",
        "            terminal = True\n",
        "            #self.__init__()\n",
        "            reward = -1\n",
        "\n",
        "        # draw sprites\n",
        "        SCREEN.blit(IMAGES['background'], (0,0))\n",
        "\n",
        "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
        "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
        "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
        "\n",
        "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
        "        # print score so player overlaps the score\n",
        "        # showScore(self.score)\n",
        "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
        "                    (self.playerx, self.playery))\n",
        "\n",
        "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
        "        pygame.display.update()\n",
        "        FPSCLOCK.tick(FPS)\n",
        "        return image_data, reward, terminal\n",
        "\n",
        "\n",
        "def getRandomPipe():\n",
        "    \"\"\"returns a randomly generated pipe\"\"\"\n",
        "    # y of gap between upper and lower pipe\n",
        "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
        "    index = random.randint(0, len(gapYs)-1)\n",
        "    gapY = gapYs[index]\n",
        "\n",
        "    gapY += int(BASEY * 0.2)\n",
        "    pipeX = SCREENWIDTH + 10\n",
        "\n",
        "    return [\n",
        "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
        "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
        "    ]\n",
        "\n",
        "\n",
        "def checkCrash(player, upperPipes, lowerPipes):\n",
        "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
        "    pi = player['index']\n",
        "    player['w'] = IMAGES['player'][0].get_width()\n",
        "    player['h'] = IMAGES['player'][0].get_height()\n",
        "\n",
        "    # if player crashes into ground\n",
        "    if player['y'] + player['h'] >= BASEY - 1:\n",
        "        return True\n",
        "    else:\n",
        "\n",
        "        playerRect = pygame.Rect(player['x'], player['y'],\n",
        "                      player['w'], player['h'])\n",
        "\n",
        "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
        "            # upper and lower pipe rects\n",
        "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
        "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
        "\n",
        "            # player and upper/lower pipe hitmasks\n",
        "            pHitMask = HITMASKS['player'][pi]\n",
        "            uHitmask = HITMASKS['pipe'][0]\n",
        "            lHitmask = HITMASKS['pipe'][1]\n",
        "\n",
        "            # if bird collided with upipe or lpipe\n",
        "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
        "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
        "\n",
        "            if uCollide or lCollide:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
        "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
        "    rect = rect1.clip(rect2)\n",
        "\n",
        "    if rect.width == 0 or rect.height == 0:\n",
        "        return False\n",
        "\n",
        "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
        "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
        "\n",
        "    for x in range(rect.width):\n",
        "        for y in range(rect.height):\n",
        "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
        "                return True\n",
        "    return False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxkGh__-wlgG"
      },
      "source": [
        "# DQN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC91QeEsfjtr"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def weights_init(layer):\n",
        "  if isinstance(layer, torch.nn.Conv2d) or isinstance(layer, torch.nn.Linear):\n",
        "    torch.nn.init.normal_(layer.weight, mean = 0., std = 0.01)\n",
        "    layer.bias.data.fill_(0.01)\n",
        "\n",
        "class DQN_net(torch.nn.Module):\n",
        "  def __init__(self, in_channels = 4, out_actions = 2):\n",
        "    super(DQN_net, self).__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels, 32, kernel_size = 8, stride = 4, padding = 2)\n",
        "    self.maxpool1 = torch.nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = torch.nn.Conv2d(32, 64, kernel_size = 4, stride = 2, padding = 1)\n",
        "    self.conv3 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.fc1 = torch.nn.Linear(1600, 512)\n",
        "    self.fc2 = torch.nn.Linear(512, out_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x = F.pad(x, (0, 4, 0, 4))  # (84, 84, 4)\n",
        "    x = self.maxpool1(F.relu(self.conv1(x)))  # (10, 10, 32)\n",
        "    #x = F.pad(x, (0, 2, 0, 2))  # (12, 12, 32)\n",
        "    x = F.relu(self.conv2(x)) # (5, 5, 64)\n",
        "    #x = F.pad(x, (0, 2, 0, 2))  # (7, 7, 64)\n",
        "    x = F.relu(self.conv3(x)) # (5, 5, 64)\n",
        "    x = x.reshape(-1, 1600)  # (1, 1600)\n",
        "    x = F.relu(self.fc1(x)) # (1, 512)\n",
        "    x = self.fc2(x) # (1, 2)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJIeJM3zVi4"
      },
      "source": [
        "# Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmj7adwazSzr"
      },
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.container = []\n",
        "  def store(self, transition):\n",
        "    self.container.append(transition)\n",
        "    if len(self.container) > self.capacity:\n",
        "      del self.container[0]\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.container, batch_size)\n",
        "  def __len__(self):\n",
        "    return len(self.container)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Ss6tzwwrAO"
      },
      "source": [
        "# DQN Training Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhWOqAQwbm9"
      },
      "source": [
        "class DQN:\n",
        "  STACK_FRAMES = 4\n",
        "  def __init__(self, memory_capacity, batch_size, epsilon, explore, replace_period, alpha, gamma, num_frames, num_actions):\n",
        "    # Hyper-parameters\n",
        "    self.replace_period = replace_period\n",
        "    self.replace_counter = 1\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_step = (epsilon - 0.0001) / explore\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # NN, loss, optimizer\n",
        "    self.policy_net = DQN_net(num_frames, num_actions).to(device)\n",
        "    self.target_net = DQN_net(num_frames, num_actions).to(device)\n",
        "    self.policy_net.apply(weights_init)\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "    self.loss_function = torch.nn.MSELoss().to(device)\n",
        "    self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr = self.alpha)\n",
        "    # Replay Memory\n",
        "    self.replay_memory = ReplayMemory(memory_capacity)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def train(self):\n",
        "    # Sample transition\n",
        "    batch = self.replay_memory.sample(self.batch_size)\n",
        "    state, action, reward, state_, terminal = zip(*batch)\n",
        "    state = torch.tensor(state, dtype = torch.float32, requires_grad = True, device = device).reshape(self.batch_size, STACK_FRAMES, 80, 80)\n",
        "    action = torch.cat(action).to(device)\n",
        "    reward = torch.tensor(reward, dtype = torch.float32, requires_grad = False, device = device).reshape(self.batch_size, 1)\n",
        "    state_ = torch.tensor(state_, dtype = torch.float32, requires_grad = False, device = device).reshape(self.batch_size, STACK_FRAMES, 80, 80)\n",
        "    # (R + gamma * Q_) - Q\n",
        "    Q = self.policy_net(state).gather(dim = 1, index = action.view(-1, 1))\n",
        "    Q_ = self.target_net(state_).max(dim = 1)[0].view(-1, 1)\n",
        "    TD_target = torch.zeros(self.batch_size, 1).to(device)\n",
        "    # G = reward + self.gamma * Q_\n",
        "    for i in range(self.batch_size):\n",
        "      if not terminal[i]:\n",
        "        TD_target[i, 0] = reward[i, 0] + self.gamma * Q_[i, 0]\n",
        "      else:\n",
        "        TD_target[i, 0] = reward[i, 0]\n",
        "    # TD_target[terminal == False, 0] = G[terminal == False, 0]\n",
        "    # TD_target[terminal == True, 0] = reward[terminal == True, 0]\n",
        "    # loss\n",
        "    loss = self.loss_function(Q, TD_target)\n",
        "    # Optimize\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    if self.replace_counter % self.replace_period == 0:\n",
        "      self.update_target_net()\n",
        "      self.replace_counter = 1\n",
        "    self.replace_counter += 1\n",
        "\n",
        "  def choose_action(self, obs, is_train = True):\n",
        "    if is_train:\n",
        "      if random.random() > self.epsilon:\n",
        "        return self.policy_net(obs).max(dim = 1)[1]\n",
        "      else:\n",
        "        return torch.tensor([random.randint(0, 1)], dtype = torch.int64, device = device)\n",
        "    else:\n",
        "      return self.policy_net(obs).max(dim = 1)[1]\n",
        "\n",
        "  def memory_store(self, transition):\n",
        "    self.replay_memory.store(transition)\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    if self.epsilon > 0.0001:\n",
        "      self.epsilon -= self.epsilon_step\n",
        "\n",
        "  def update_target_net(self):\n",
        "    print('Update Target Net')\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "  def load_model(self, PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "    self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    self.epsilon = checkpoint['epsilon']\n",
        "    return checkpoint['episode'], checkpoint['iterations']\n",
        "\n",
        "  def save_model(self, episode, iterations):\n",
        "    torch.save({\n",
        "        'episode': episode,\n",
        "        'iterations': iterations,\n",
        "        'policy_net_state_dict': self.policy_net.state_dict(),\n",
        "        'target_net_state_dict': self.target_net.state_dict(),\n",
        "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        'epsilon': self.epsilon\n",
        "    }, '/content/checkpoint' + str(episode) + '.tar')\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6upBRxCmc0A"
      },
      "source": [
        "# DQN Process\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn9vR6k6lv3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2584d0-31a0-47bd-a46e-c2ceb010260f"
      },
      "source": [
        "OBSERVE = 100\n",
        "EXPLORE = 100000.\n",
        "EPISODE = 1000000\n",
        "ACTION_IDLE = 1\n",
        "SAVE_ITER = 1000\n",
        "STACK_FRAMES = 4\n",
        "LOAD = True\n",
        "\n",
        "random.seed(42)\n",
        "# Initialize Game\n",
        "game = GameState()\n",
        "episode = 1\n",
        "iterations = 0\n",
        "# Initialize Model\n",
        "if not LOAD:\n",
        "  dqn = DQN(memory_capacity = 50000,\n",
        "            batch_size = 32,\n",
        "            epsilon = 0.2,\n",
        "            explore = EXPLORE,\n",
        "            replace_period = 100,\n",
        "            alpha = 1e-6,\n",
        "            gamma = 0.99,\n",
        "            num_frames = STACK_FRAMES,\n",
        "            num_actions = 2)\n",
        "elif LOAD:\n",
        "  dqn = DQN(memory_capacity = 50000,\n",
        "            batch_size = 32,\n",
        "            epsilon = 0.0001,\n",
        "            explore = EXPLORE,\n",
        "            replace_period = 100,\n",
        "            alpha = 1e-6,\n",
        "            gamma = 0.99,\n",
        "            num_frames = STACK_FRAMES,\n",
        "            num_actions = 2)\n",
        "  # Populate\n",
        "  ckpts = [5295, 5329, 5370, 5409, 5438, 5470, 5498, 5532, 5569, 5604, 5643, 5682]\n",
        "  for ckpt in ckpts:\n",
        "    dqn.load_model('/content/checkpoint' + str(ckpt) + '.tar')\n",
        "    print('Start Populating by Model: ', ckpt)\n",
        "    for ep in range(15):\n",
        "      game.__init__()\n",
        "      R = 0\n",
        "      obs, reward, terminal = game.frame_step(np.array([1, 0]))\n",
        "      obs = cv2.cvtColor(cv2.resize(obs, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "      _, obs = cv2.threshold(obs, 1, 255, cv2.THRESH_BINARY)\n",
        "      obs = np.reshape(obs, (1, 80, 80))\n",
        "      obs = np.concatenate([obs] * STACK_FRAMES, axis = 0)\n",
        "      while not terminal:\n",
        "        # Choose actions\n",
        "        if iterations % ACTION_IDLE == 0:\n",
        "          obs_tmp = torch.tensor(obs, dtype = torch.float32, device = device).reshape(1, STACK_FRAMES, 80, 80)\n",
        "          action = dqn.choose_action(obs_tmp, False)\n",
        "        else:\n",
        "          action = torch.tensor(0, dtype = torch.int64, device = device)\n",
        "        # Get next state\n",
        "        if action.cpu().numpy()[0] == 0:\n",
        "          act = np.array([1, 0])\n",
        "        elif action.cpu().numpy()[0] == 1:\n",
        "          act = np.array([0, 1])\n",
        "        obs_, reward, terminal = game.frame_step(act)\n",
        "        obs_ = cv2.cvtColor(cv2.resize(obs_, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "        _, obs_ = cv2.threshold(obs_, 1, 255, cv2.THRESH_BINARY)\n",
        "        obs_ = np.reshape(obs_, (1, 80, 80))\n",
        "        obs_ = np.concatenate([obs_, obs[:3, ...]], axis = 0)\n",
        "        # Push transition to replay memory\n",
        "        transition = [obs, action, reward, obs_, terminal]\n",
        "        dqn.memory_store(transition)  \n",
        "        # Update\n",
        "        obs = obs_\n",
        "        R += reward\n",
        "      print('Episode: {}, Total Reward: {}'.format(ep, R))\n",
        "  print('Start training by Model: ', 5721)\n",
        "  episode, iterations = dqn.load_model('/content/checkpoint' + str(5721) + '.tar')\n",
        "\n",
        "\n",
        "while episode <= EPISODE:\n",
        "  game.__init__()\n",
        "  R = 0\n",
        "  # Get the first frame and stack it 4 times\n",
        "  obs, reward, terminal = game.frame_step(np.array([1, 0]))\n",
        "  obs = cv2.cvtColor(cv2.resize(obs, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "  _, obs = cv2.threshold(obs, 1, 255, cv2.THRESH_BINARY)\n",
        "  obs = np.reshape(obs, (1, 80, 80))\n",
        "  obs = np.concatenate([obs] * STACK_FRAMES, axis = 0)\n",
        "  while not terminal:\n",
        "    # Choose actions\n",
        "    if iterations % ACTION_IDLE == 0:\n",
        "      obs_tmp = torch.tensor(obs, dtype = torch.float32, device = device).reshape(1, STACK_FRAMES, 80, 80)\n",
        "      action = dqn.choose_action(obs_tmp)\n",
        "    else:\n",
        "      action = torch.tensor(0, dtype = torch.int64, device = device)\n",
        "\n",
        "    # Get next state\n",
        "    if action.cpu().numpy()[0] == 0:\n",
        "      act = np.array([1, 0])\n",
        "    elif action.cpu().numpy()[0] == 1:\n",
        "      act = np.array([0, 1])\n",
        "    obs_, reward, terminal = game.frame_step(act)\n",
        "    obs_ = cv2.cvtColor(cv2.resize(obs_, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "    _, obs_ = cv2.threshold(obs_, 1, 255, cv2.THRESH_BINARY)\n",
        "    obs_ = np.reshape(obs_, (1, 80, 80))\n",
        "    obs_ = np.concatenate([obs_, obs[:3, ...]], axis = 0)\n",
        "\n",
        "    # Push transition to replay memory\n",
        "    transition = [obs, action, reward, obs_, terminal]\n",
        "    dqn.memory_store(transition)\n",
        "\n",
        "    # Train\n",
        "    if iterations > OBSERVE:\n",
        "      dqn.update_epsilon()\n",
        "      dqn.train()\n",
        "    \n",
        "    # Update\n",
        "    obs = obs_\n",
        "    iterations += 1\n",
        "    R += reward\n",
        "    if iterations % SAVE_ITER  == 0 and iterations > OBSERVE:\n",
        "      dqn.save_model(episode, iterations)\n",
        "  \n",
        "  test_img = np.transpose(obs, (1, 2, 0))\n",
        "  cv2.imwrite('/content/test1.png', test_img[..., 0])\n",
        "  cv2.imwrite('/content/test2.png', test_img[..., 1])\n",
        "  cv2.imwrite('/content/test3.png', test_img[..., 2])\n",
        "  cv2.imwrite('/content/test4.png', test_img[..., 3])\n",
        "  print('Episode: {}, Total Reward: {}, Iterations: {}, Epsilon: {}, Memory Size: {}'.format(episode, R, iterations, dqn.epsilon, len(dqn.replay_memory)))\n",
        "  episode += 1\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Populating by Model:  5295\n",
            "Episode: 0, Total Reward: 18.500000000000018\n",
            "Episode: 1, Total Reward: 8.999999999999984\n",
            "Episode: 2, Total Reward: 2.700000000000002\n",
            "Episode: 3, Total Reward: 22.50000000000006\n",
            "Episode: 4, Total Reward: 3.8999999999999986\n",
            "Episode: 5, Total Reward: 2.700000000000002\n",
            "Episode: 6, Total Reward: 3.8999999999999986\n",
            "Episode: 7, Total Reward: 9.199999999999983\n",
            "Episode: 8, Total Reward: 6.099999999999994\n",
            "Episode: 9, Total Reward: 8.899999999999984\n",
            "Episode: 10, Total Reward: 4.4999999999999964\n",
            "Episode: 11, Total Reward: 9.399999999999983\n",
            "Episode: 12, Total Reward: 8.899999999999984\n",
            "Episode: 13, Total Reward: 4.399999999999997\n",
            "Episode: 14, Total Reward: 10.799999999999981\n",
            "Start Populating by Model:  5329\n",
            "Episode: 0, Total Reward: 22.200000000000056\n",
            "Episode: 1, Total Reward: 4.1999999999999975\n",
            "Episode: 2, Total Reward: 5.899999999999995\n",
            "Episode: 3, Total Reward: 4.799999999999995\n",
            "Episode: 4, Total Reward: 13.399999999999972\n",
            "Episode: 5, Total Reward: 10.799999999999981\n",
            "Episode: 6, Total Reward: 3.799999999999999\n",
            "Episode: 7, Total Reward: 27.10000000000011\n",
            "Episode: 8, Total Reward: 19.60000000000002\n",
            "Episode: 9, Total Reward: 3.799999999999999\n",
            "Episode: 10, Total Reward: 3.799999999999999\n",
            "Episode: 11, Total Reward: 9.299999999999983\n",
            "Episode: 12, Total Reward: 4.1999999999999975\n",
            "Episode: 13, Total Reward: 3.799999999999999\n",
            "Episode: 14, Total Reward: 10.799999999999981\n",
            "Start Populating by Model:  5370\n",
            "Episode: 0, Total Reward: 6.199999999999994\n",
            "Episode: 1, Total Reward: 6.099999999999994\n",
            "Episode: 2, Total Reward: 4.299999999999997\n",
            "Episode: 3, Total Reward: 8.299999999999986\n",
            "Episode: 4, Total Reward: 3.799999999999999\n",
            "Episode: 5, Total Reward: 13.199999999999973\n",
            "Episode: 6, Total Reward: 6.399999999999993\n",
            "Episode: 7, Total Reward: 3.799999999999999\n",
            "Episode: 8, Total Reward: 3.799999999999999\n",
            "Episode: 9, Total Reward: 6.199999999999994\n",
            "Episode: 10, Total Reward: 6.399999999999993\n",
            "Episode: 11, Total Reward: 9.099999999999984\n",
            "Episode: 12, Total Reward: 8.999999999999984\n",
            "Episode: 13, Total Reward: 3.799999999999999\n",
            "Episode: 14, Total Reward: 6.399999999999993\n",
            "Start Populating by Model:  5409\n",
            "Episode: 0, Total Reward: 22.800000000000065\n",
            "Episode: 1, Total Reward: 3.9999999999999982\n",
            "Episode: 2, Total Reward: 6.299999999999994\n",
            "Episode: 3, Total Reward: 3.9999999999999982\n",
            "Episode: 4, Total Reward: 4.1999999999999975\n",
            "Episode: 5, Total Reward: 13.79999999999997\n",
            "Episode: 6, Total Reward: 5.999999999999995\n",
            "Episode: 7, Total Reward: 3.9999999999999982\n",
            "Episode: 8, Total Reward: 13.499999999999972\n",
            "Episode: 9, Total Reward: 23.30000000000007\n",
            "Episode: 10, Total Reward: 19.60000000000002\n",
            "Episode: 11, Total Reward: 9.099999999999984\n",
            "Episode: 12, Total Reward: 8.799999999999985\n",
            "Episode: 13, Total Reward: 8.299999999999986\n",
            "Episode: 14, Total Reward: 13.299999999999972\n",
            "Start Populating by Model:  5438\n",
            "Episode: 0, Total Reward: 6.099999999999994\n",
            "Episode: 1, Total Reward: 3.799999999999999\n",
            "Episode: 2, Total Reward: 9.299999999999983\n",
            "Episode: 3, Total Reward: 3.799999999999999\n",
            "Episode: 4, Total Reward: 18.200000000000014\n",
            "Episode: 5, Total Reward: 8.799999999999985\n",
            "Episode: 6, Total Reward: 4.099999999999998\n",
            "Episode: 7, Total Reward: 8.399999999999986\n",
            "Episode: 8, Total Reward: 18.100000000000012\n",
            "Episode: 9, Total Reward: 8.399999999999986\n",
            "Episode: 10, Total Reward: 3.799999999999999\n",
            "Episode: 11, Total Reward: 10.799999999999981\n",
            "Episode: 12, Total Reward: 3.8999999999999986\n",
            "Episode: 13, Total Reward: 4.099999999999998\n",
            "Episode: 14, Total Reward: 3.799999999999999\n",
            "Start Populating by Model:  5470\n",
            "Episode: 0, Total Reward: 3.9999999999999982\n",
            "Episode: 1, Total Reward: 38.50000000000023\n",
            "Episode: 2, Total Reward: 3.8999999999999986\n",
            "Episode: 3, Total Reward: 9.199999999999983\n",
            "Episode: 4, Total Reward: 6.399999999999993\n",
            "Episode: 5, Total Reward: 9.199999999999983\n",
            "Episode: 6, Total Reward: 12.999999999999973\n",
            "Episode: 7, Total Reward: 26.800000000000107\n",
            "Episode: 8, Total Reward: 8.999999999999984\n",
            "Episode: 9, Total Reward: 6.099999999999994\n",
            "Episode: 10, Total Reward: 4.799999999999995\n",
            "Episode: 11, Total Reward: 6.399999999999993\n",
            "Episode: 12, Total Reward: 13.399999999999972\n",
            "Episode: 13, Total Reward: 8.799999999999985\n",
            "Episode: 14, Total Reward: 4.299999999999997\n",
            "Start Populating by Model:  5498\n",
            "Episode: 0, Total Reward: 3.799999999999999\n",
            "Episode: 1, Total Reward: 10.799999999999981\n",
            "Episode: 2, Total Reward: 9.199999999999983\n",
            "Episode: 3, Total Reward: 4.4999999999999964\n",
            "Episode: 4, Total Reward: 26.800000000000107\n",
            "Episode: 5, Total Reward: 13.399999999999972\n",
            "Episode: 6, Total Reward: 13.99999999999997\n",
            "Episode: 7, Total Reward: 4.699999999999996\n",
            "Episode: 8, Total Reward: 8.299999999999986\n",
            "Episode: 9, Total Reward: 8.999999999999984\n",
            "Episode: 10, Total Reward: 17.90000000000001\n",
            "Episode: 11, Total Reward: 23.10000000000007\n",
            "Episode: 12, Total Reward: 13.099999999999973\n",
            "Episode: 13, Total Reward: 3.799999999999999\n",
            "Episode: 14, Total Reward: 13.199999999999973\n",
            "Start Populating by Model:  5532\n",
            "Episode: 0, Total Reward: 8.799999999999985\n",
            "Episode: 1, Total Reward: 4.099999999999998\n",
            "Episode: 2, Total Reward: 4.699999999999996\n",
            "Episode: 3, Total Reward: 9.099999999999984\n",
            "Episode: 4, Total Reward: 3.799999999999999\n",
            "Episode: 5, Total Reward: 8.299999999999986\n",
            "Episode: 6, Total Reward: 26.800000000000107\n",
            "Episode: 7, Total Reward: 4.699999999999996\n",
            "Episode: 8, Total Reward: 5.899999999999995\n",
            "Episode: 9, Total Reward: 3.9999999999999982\n",
            "Episode: 10, Total Reward: 6.499999999999993\n",
            "Episode: 11, Total Reward: 6.099999999999994\n",
            "Episode: 12, Total Reward: 13.399999999999972\n",
            "Episode: 13, Total Reward: 10.499999999999982\n",
            "Episode: 14, Total Reward: 8.799999999999985\n",
            "Start Populating by Model:  5569\n",
            "Episode: 0, Total Reward: 10.799999999999981\n",
            "Episode: 1, Total Reward: 17.800000000000008\n",
            "Episode: 2, Total Reward: 6.299999999999994\n",
            "Episode: 3, Total Reward: 15.19999999999997\n",
            "Episode: 4, Total Reward: 6.299999999999994\n",
            "Episode: 5, Total Reward: 6.399999999999993\n",
            "Episode: 6, Total Reward: 8.399999999999986\n",
            "Episode: 7, Total Reward: 10.89999999999998\n",
            "Episode: 8, Total Reward: 22.300000000000058\n",
            "Episode: 9, Total Reward: 3.799999999999999\n",
            "Episode: 10, Total Reward: 3.799999999999999\n",
            "Episode: 11, Total Reward: 3.799999999999999\n",
            "Episode: 12, Total Reward: 4.899999999999995\n",
            "Episode: 13, Total Reward: 6.299999999999994\n",
            "Episode: 14, Total Reward: 8.899999999999984\n",
            "Start Populating by Model:  5604\n",
            "Episode: 0, Total Reward: 10.89999999999998\n",
            "Episode: 1, Total Reward: 12.899999999999974\n",
            "Episode: 2, Total Reward: 3.799999999999999\n",
            "Episode: 3, Total Reward: 8.399999999999986\n",
            "Episode: 4, Total Reward: 3.799999999999999\n",
            "Episode: 5, Total Reward: 13.199999999999973\n",
            "Episode: 6, Total Reward: 10.89999999999998\n",
            "Episode: 7, Total Reward: 13.79999999999997\n",
            "Episode: 8, Total Reward: 4.599999999999996\n",
            "Episode: 9, Total Reward: 6.199999999999994\n",
            "Episode: 10, Total Reward: 3.799999999999999\n",
            "Episode: 11, Total Reward: 4.599999999999996\n",
            "Episode: 12, Total Reward: 3.799999999999999\n",
            "Episode: 13, Total Reward: 8.699999999999985\n",
            "Episode: 14, Total Reward: 8.599999999999985\n",
            "Start Populating by Model:  5643\n",
            "Episode: 0, Total Reward: 6.199999999999994\n",
            "Episode: 1, Total Reward: 8.699999999999985\n",
            "Episode: 2, Total Reward: 6.199999999999994\n",
            "Episode: 3, Total Reward: 6.099999999999994\n",
            "Episode: 4, Total Reward: 8.699999999999985\n",
            "Episode: 5, Total Reward: 6.299999999999994\n",
            "Episode: 6, Total Reward: 3.9999999999999982\n",
            "Episode: 7, Total Reward: 6.199999999999994\n",
            "Episode: 8, Total Reward: 17.800000000000008\n",
            "Episode: 9, Total Reward: 4.699999999999996\n",
            "Episode: 10, Total Reward: 3.9999999999999982\n",
            "Episode: 11, Total Reward: 12.899999999999974\n",
            "Episode: 12, Total Reward: 8.299999999999986\n",
            "Episode: 13, Total Reward: 4.699999999999996\n",
            "Episode: 14, Total Reward: 3.9999999999999982\n",
            "Start Populating by Model:  5682\n",
            "Episode: 0, Total Reward: 3.799999999999999\n",
            "Episode: 1, Total Reward: 3.799999999999999\n",
            "Episode: 2, Total Reward: 3.799999999999999\n",
            "Episode: 3, Total Reward: 4.599999999999996\n",
            "Episode: 4, Total Reward: 4.899999999999995\n",
            "Episode: 5, Total Reward: 3.799999999999999\n",
            "Episode: 6, Total Reward: 3.799999999999999\n",
            "Episode: 7, Total Reward: 12.999999999999973\n",
            "Episode: 8, Total Reward: 8.299999999999986\n",
            "Episode: 9, Total Reward: 6.299999999999994\n",
            "Episode: 10, Total Reward: 10.399999999999983\n",
            "Episode: 11, Total Reward: 8.799999999999985\n",
            "Episode: 12, Total Reward: 5.999999999999995\n",
            "Episode: 13, Total Reward: 8.699999999999985\n",
            "Episode: 14, Total Reward: 20.100000000000026\n",
            "Start training by Model:  5721\n",
            "Episode: 5721, Total Reward: 10.599999999999982, Iterations: 364099, Epsilon: 9.999999979125668e-05, Memory Size: 16045\n",
            "Update Target Net\n",
            "Episode: 5722, Total Reward: 3.799999999999999, Iterations: 364148, Epsilon: 9.999999979125668e-05, Memory Size: 16094\n",
            "Update Target Net\n",
            "Episode: 5723, Total Reward: 4.799999999999995, Iterations: 364207, Epsilon: 9.999999979125668e-05, Memory Size: 16153\n",
            "Update Target Net\n",
            "Episode: 5724, Total Reward: 15.09999999999997, Iterations: 364342, Epsilon: 9.999999979125668e-05, Memory Size: 16288\n",
            "Episode: 5725, Total Reward: 3.799999999999999, Iterations: 364391, Epsilon: 9.999999979125668e-05, Memory Size: 16337\n",
            "Update Target Net\n",
            "Episode: 5726, Total Reward: 8.999999999999984, Iterations: 364483, Epsilon: 9.999999979125668e-05, Memory Size: 16429\n",
            "Update Target Net\n",
            "Episode: 5727, Total Reward: 5.999999999999995, Iterations: 364545, Epsilon: 9.999999979125668e-05, Memory Size: 16491\n",
            "Update Target Net\n",
            "Episode: 5728, Total Reward: 4.4999999999999964, Iterations: 364601, Epsilon: 9.999999979125668e-05, Memory Size: 16547\n",
            "Episode: 5729, Total Reward: 5.999999999999995, Iterations: 364663, Epsilon: 9.999999979125668e-05, Memory Size: 16609\n",
            "Update Target Net\n",
            "Episode: 5730, Total Reward: 13.099999999999973, Iterations: 364787, Epsilon: 9.999999979125668e-05, Memory Size: 16733\n",
            "Update Target Net\n",
            "Episode: 5731, Total Reward: 6.299999999999994, Iterations: 364852, Epsilon: 9.999999979125668e-05, Memory Size: 16798\n",
            "Update Target Net\n",
            "Episode: 5732, Total Reward: 3.799999999999999, Iterations: 364901, Epsilon: 9.999999979125668e-05, Memory Size: 16847\n",
            "Episode: 5733, Total Reward: 4.699999999999996, Iterations: 364959, Epsilon: 9.999999979125668e-05, Memory Size: 16905\n",
            "Update Target Net\n",
            "Episode: 5734, Total Reward: 3.799999999999999, Iterations: 365008, Epsilon: 9.999999979125668e-05, Memory Size: 16954\n",
            "Update Target Net\n",
            "Episode: 5735, Total Reward: 10.699999999999982, Iterations: 365108, Epsilon: 9.999999979125668e-05, Memory Size: 17054\n",
            "Episode: 5736, Total Reward: 3.8999999999999986, Iterations: 365158, Epsilon: 9.999999979125668e-05, Memory Size: 17104\n",
            "Update Target Net\n",
            "Episode: 5737, Total Reward: 4.799999999999995, Iterations: 365217, Epsilon: 9.999999979125668e-05, Memory Size: 17163\n",
            "Episode: 5738, Total Reward: 5.899999999999995, Iterations: 365278, Epsilon: 9.999999979125668e-05, Memory Size: 17224\n",
            "Update Target Net\n",
            "Episode: 5739, Total Reward: 8.299999999999986, Iterations: 365363, Epsilon: 9.999999979125668e-05, Memory Size: 17309\n",
            "Update Target Net\n",
            "Episode: 5740, Total Reward: 10.399999999999983, Iterations: 365460, Epsilon: 9.999999979125668e-05, Memory Size: 17406\n",
            "Update Target Net\n",
            "Episode: 5741, Total Reward: 6.299999999999994, Iterations: 365525, Epsilon: 9.999999979125668e-05, Memory Size: 17471\n",
            "Update Target Net\n",
            "Episode: 5742, Total Reward: 12.899999999999974, Iterations: 365647, Epsilon: 9.999999979125668e-05, Memory Size: 17593\n",
            "Update Target Net\n",
            "Episode: 5743, Total Reward: 6.399999999999993, Iterations: 365713, Epsilon: 9.999999979125668e-05, Memory Size: 17659\n",
            "Episode: 5744, Total Reward: 3.8999999999999986, Iterations: 365763, Epsilon: 9.999999979125668e-05, Memory Size: 17709\n",
            "Update Target Net\n",
            "Episode: 5745, Total Reward: 4.299999999999997, Iterations: 365817, Epsilon: 9.999999979125668e-05, Memory Size: 17763\n",
            "Episode: 5746, Total Reward: 4.899999999999995, Iterations: 365877, Epsilon: 9.999999979125668e-05, Memory Size: 17823\n",
            "Update Target Net\n",
            "Episode: 5747, Total Reward: 8.899999999999984, Iterations: 365968, Epsilon: 9.999999979125668e-05, Memory Size: 17914\n",
            "Update Target Net\n",
            "Episode: 5748, Total Reward: 8.299999999999986, Iterations: 366053, Epsilon: 9.999999979125668e-05, Memory Size: 17999\n",
            "Update Target Net\n",
            "Episode: 5749, Total Reward: 10.99999999999998, Iterations: 366156, Epsilon: 9.999999979125668e-05, Memory Size: 18102\n",
            "Update Target Net\n",
            "Episode: 5750, Total Reward: 3.799999999999999, Iterations: 366205, Epsilon: 9.999999979125668e-05, Memory Size: 18151\n",
            "Episode: 5751, Total Reward: 6.199999999999994, Iterations: 366269, Epsilon: 9.999999979125668e-05, Memory Size: 18215\n",
            "Update Target Net\n",
            "Episode: 5752, Total Reward: 6.299999999999994, Iterations: 366334, Epsilon: 9.999999979125668e-05, Memory Size: 18280\n",
            "Update Target Net\n",
            "Episode: 5753, Total Reward: 5.999999999999995, Iterations: 366396, Epsilon: 9.999999979125668e-05, Memory Size: 18342\n",
            "Episode: 5754, Total Reward: 6.099999999999994, Iterations: 366459, Epsilon: 9.999999979125668e-05, Memory Size: 18405\n",
            "Update Target Net\n",
            "Episode: 5755, Total Reward: 4.699999999999996, Iterations: 366517, Epsilon: 9.999999979125668e-05, Memory Size: 18463\n",
            "Episode: 5756, Total Reward: 4.599999999999996, Iterations: 366574, Epsilon: 9.999999979125668e-05, Memory Size: 18520\n",
            "Update Target Net\n",
            "Episode: 5757, Total Reward: 8.499999999999986, Iterations: 366661, Epsilon: 9.999999979125668e-05, Memory Size: 18607\n",
            "Update Target Net\n",
            "Episode: 5758, Total Reward: 3.799999999999999, Iterations: 366710, Epsilon: 9.999999979125668e-05, Memory Size: 18656\n",
            "Update Target Net\n",
            "Episode: 5759, Total Reward: 9.199999999999983, Iterations: 366804, Epsilon: 9.999999979125668e-05, Memory Size: 18750\n",
            "Episode: 5760, Total Reward: 3.9999999999999982, Iterations: 366855, Epsilon: 9.999999979125668e-05, Memory Size: 18801\n",
            "Update Target Net\n",
            "Episode: 5761, Total Reward: 6.299999999999994, Iterations: 366920, Epsilon: 9.999999979125668e-05, Memory Size: 18866\n",
            "Update Target Net\n",
            "Episode: 5762, Total Reward: 10.599999999999982, Iterations: 367019, Epsilon: 9.999999979125668e-05, Memory Size: 18965\n",
            "Episode: 5763, Total Reward: 3.799999999999999, Iterations: 367068, Epsilon: 9.999999979125668e-05, Memory Size: 19014\n",
            "Update Target Net\n",
            "Episode: 5764, Total Reward: 4.799999999999995, Iterations: 367127, Epsilon: 9.999999979125668e-05, Memory Size: 19073\n",
            "Update Target Net\n",
            "Episode: 5765, Total Reward: 6.399999999999993, Iterations: 367193, Epsilon: 9.999999979125668e-05, Memory Size: 19139\n",
            "Episode: 5766, Total Reward: 5.999999999999995, Iterations: 367255, Epsilon: 9.999999979125668e-05, Memory Size: 19201\n",
            "Update Target Net\n",
            "Episode: 5767, Total Reward: 6.299999999999994, Iterations: 367320, Epsilon: 9.999999979125668e-05, Memory Size: 19266\n",
            "Update Target Net\n",
            "Episode: 5768, Total Reward: 3.9999999999999982, Iterations: 367371, Epsilon: 9.999999979125668e-05, Memory Size: 19317\n",
            "Episode: 5769, Total Reward: 3.9999999999999982, Iterations: 367422, Epsilon: 9.999999979125668e-05, Memory Size: 19368\n",
            "Update Target Net\n",
            "Episode: 5770, Total Reward: 9.099999999999984, Iterations: 367515, Epsilon: 9.999999979125668e-05, Memory Size: 19461\n",
            "Update Target Net\n",
            "Episode: 5771, Total Reward: 8.299999999999986, Iterations: 367600, Epsilon: 9.999999979125668e-05, Memory Size: 19546\n",
            "Update Target Net\n",
            "Episode: 5772, Total Reward: 6.299999999999994, Iterations: 367665, Epsilon: 9.999999979125668e-05, Memory Size: 19611\n",
            "Episode: 5773, Total Reward: 4.799999999999995, Iterations: 367724, Epsilon: 9.999999979125668e-05, Memory Size: 19670\n",
            "Update Target Net\n",
            "Episode: 5774, Total Reward: 6.099999999999994, Iterations: 367787, Epsilon: 9.999999979125668e-05, Memory Size: 19733\n",
            "Episode: 5775, Total Reward: 3.9999999999999982, Iterations: 367838, Epsilon: 9.999999979125668e-05, Memory Size: 19784\n",
            "Update Target Net\n",
            "Episode: 5776, Total Reward: 4.899999999999995, Iterations: 367898, Epsilon: 9.999999979125668e-05, Memory Size: 19844\n",
            "Update Target Net\n",
            "Episode: 5777, Total Reward: 6.399999999999993, Iterations: 367964, Epsilon: 9.999999979125668e-05, Memory Size: 19910\n",
            "Update Target Net\n",
            "Episode: 5778, Total Reward: 15.499999999999975, Iterations: 368103, Epsilon: 9.999999979125668e-05, Memory Size: 20049\n",
            "Update Target Net\n",
            "Episode: 5779, Total Reward: 4.899999999999995, Iterations: 368163, Epsilon: 9.999999979125668e-05, Memory Size: 20109\n",
            "Episode: 5780, Total Reward: 4.899999999999995, Iterations: 368223, Epsilon: 9.999999979125668e-05, Memory Size: 20169\n",
            "Update Target Net\n",
            "Episode: 5781, Total Reward: 3.799999999999999, Iterations: 368272, Epsilon: 9.999999979125668e-05, Memory Size: 20218\n",
            "Episode: 5782, Total Reward: 4.799999999999995, Iterations: 368331, Epsilon: 9.999999979125668e-05, Memory Size: 20277\n",
            "Update Target Net\n",
            "Episode: 5783, Total Reward: 3.799999999999999, Iterations: 368380, Epsilon: 9.999999979125668e-05, Memory Size: 20326\n",
            "Episode: 5784, Total Reward: 3.799999999999999, Iterations: 368429, Epsilon: 9.999999979125668e-05, Memory Size: 20375\n",
            "Update Target Net\n",
            "Episode: 5785, Total Reward: 5.899999999999995, Iterations: 368490, Epsilon: 9.999999979125668e-05, Memory Size: 20436\n",
            "Update Target Net\n",
            "Episode: 5786, Total Reward: 10.99999999999998, Iterations: 368593, Epsilon: 9.999999979125668e-05, Memory Size: 20539\n",
            "Update Target Net\n",
            "Episode: 5787, Total Reward: 15.299999999999972, Iterations: 368730, Epsilon: 9.999999979125668e-05, Memory Size: 20676\n",
            "Update Target Net\n",
            "Episode: 5788, Total Reward: 4.899999999999995, Iterations: 368790, Epsilon: 9.999999979125668e-05, Memory Size: 20736\n",
            "Episode: 5789, Total Reward: 3.9999999999999982, Iterations: 368841, Epsilon: 9.999999979125668e-05, Memory Size: 20787\n",
            "Update Target Net\n",
            "Episode: 5790, Total Reward: 4.799999999999995, Iterations: 368900, Epsilon: 9.999999979125668e-05, Memory Size: 20846\n",
            "Update Target Net\n",
            "Episode: 5791, Total Reward: 8.299999999999986, Iterations: 368985, Epsilon: 9.999999979125668e-05, Memory Size: 20931\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5792, Total Reward: 19.900000000000023, Iterations: 369159, Epsilon: 9.999999979125668e-05, Memory Size: 21105\n",
            "Episode: 5793, Total Reward: 8.299999999999986, Iterations: 369244, Epsilon: 9.999999979125668e-05, Memory Size: 21190\n",
            "Update Target Net\n",
            "Episode: 5794, Total Reward: 10.599999999999982, Iterations: 369343, Epsilon: 9.999999979125668e-05, Memory Size: 21289\n",
            "Update Target Net\n",
            "Episode: 5795, Total Reward: 8.299999999999986, Iterations: 369428, Epsilon: 9.999999979125668e-05, Memory Size: 21374\n",
            "Update Target Net\n",
            "Episode: 5796, Total Reward: 4.899999999999995, Iterations: 369488, Epsilon: 9.999999979125668e-05, Memory Size: 21434\n",
            "Update Target Net\n",
            "Episode: 5797, Total Reward: 6.199999999999994, Iterations: 369552, Epsilon: 9.999999979125668e-05, Memory Size: 21498\n",
            "Episode: 5798, Total Reward: 4.699999999999996, Iterations: 369610, Epsilon: 9.999999979125668e-05, Memory Size: 21556\n",
            "Update Target Net\n",
            "Episode: 5799, Total Reward: 9.099999999999984, Iterations: 369703, Epsilon: 9.999999979125668e-05, Memory Size: 21649\n",
            "Update Target Net\n",
            "Episode: 5800, Total Reward: 3.799999999999999, Iterations: 369752, Epsilon: 9.999999979125668e-05, Memory Size: 21698\n",
            "Episode: 5801, Total Reward: 5.999999999999995, Iterations: 369814, Epsilon: 9.999999979125668e-05, Memory Size: 21760\n",
            "Update Target Net\n",
            "Episode: 5802, Total Reward: 9.099999999999984, Iterations: 369907, Epsilon: 9.999999979125668e-05, Memory Size: 21853\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5803, Total Reward: 17.600000000000005, Iterations: 370067, Epsilon: 9.999999979125668e-05, Memory Size: 22013\n",
            "Update Target Net\n",
            "Episode: 5804, Total Reward: 8.499999999999986, Iterations: 370154, Epsilon: 9.999999979125668e-05, Memory Size: 22100\n",
            "Episode: 5805, Total Reward: 6.199999999999994, Iterations: 370218, Epsilon: 9.999999979125668e-05, Memory Size: 22164\n",
            "Update Target Net\n",
            "Episode: 5806, Total Reward: 8.899999999999984, Iterations: 370309, Epsilon: 9.999999979125668e-05, Memory Size: 22255\n",
            "Update Target Net\n",
            "Episode: 5807, Total Reward: 4.899999999999995, Iterations: 370369, Epsilon: 9.999999979125668e-05, Memory Size: 22315\n",
            "Update Target Net\n",
            "Episode: 5808, Total Reward: 10.799999999999981, Iterations: 370470, Epsilon: 9.999999979125668e-05, Memory Size: 22416\n",
            "Episode: 5809, Total Reward: 6.099999999999994, Iterations: 370533, Epsilon: 9.999999979125668e-05, Memory Size: 22479\n",
            "Update Target Net\n",
            "Episode: 5810, Total Reward: 6.199999999999994, Iterations: 370597, Epsilon: 9.999999979125668e-05, Memory Size: 22543\n",
            "Update Target Net\n",
            "Episode: 5811, Total Reward: 4.799999999999995, Iterations: 370656, Epsilon: 9.999999979125668e-05, Memory Size: 22602\n",
            "Episode: 5812, Total Reward: 6.199999999999994, Iterations: 370720, Epsilon: 9.999999979125668e-05, Memory Size: 22666\n",
            "Update Target Net\n",
            "Episode: 5813, Total Reward: 6.399999999999993, Iterations: 370786, Epsilon: 9.999999979125668e-05, Memory Size: 22732\n",
            "Update Target Net\n",
            "Episode: 5814, Total Reward: 6.299999999999994, Iterations: 370851, Epsilon: 9.999999979125668e-05, Memory Size: 22797\n",
            "Episode: 5815, Total Reward: 6.099999999999994, Iterations: 370914, Epsilon: 9.999999979125668e-05, Memory Size: 22860\n",
            "Update Target Net\n",
            "Episode: 5816, Total Reward: 10.699999999999982, Iterations: 371014, Epsilon: 9.999999979125668e-05, Memory Size: 22960\n",
            "Update Target Net\n",
            "Episode: 5817, Total Reward: 6.399999999999993, Iterations: 371080, Epsilon: 9.999999979125668e-05, Memory Size: 23026\n",
            "Update Target Net\n",
            "Episode: 5818, Total Reward: 10.799999999999981, Iterations: 371181, Epsilon: 9.999999979125668e-05, Memory Size: 23127\n",
            "Update Target Net\n",
            "Episode: 5819, Total Reward: 4.899999999999995, Iterations: 371241, Epsilon: 9.999999979125668e-05, Memory Size: 23187\n",
            "Episode: 5820, Total Reward: 4.799999999999995, Iterations: 371300, Epsilon: 9.999999979125668e-05, Memory Size: 23246\n",
            "Update Target Net\n",
            "Episode: 5821, Total Reward: 3.799999999999999, Iterations: 371349, Epsilon: 9.999999979125668e-05, Memory Size: 23295\n",
            "Episode: 5822, Total Reward: 6.5999999999999925, Iterations: 371417, Epsilon: 9.999999979125668e-05, Memory Size: 23363\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5823, Total Reward: 24.600000000000076, Iterations: 371629, Epsilon: 9.999999979125668e-05, Memory Size: 23575\n",
            "Episode: 5824, Total Reward: 5.999999999999995, Iterations: 371691, Epsilon: 9.999999979125668e-05, Memory Size: 23637\n",
            "Update Target Net\n",
            "Episode: 5825, Total Reward: 8.399999999999986, Iterations: 371777, Epsilon: 9.999999979125668e-05, Memory Size: 23723\n",
            "Update Target Net\n",
            "Episode: 5826, Total Reward: 10.499999999999982, Iterations: 371875, Epsilon: 9.999999979125668e-05, Memory Size: 23821\n",
            "Update Target Net\n",
            "Episode: 5827, Total Reward: 6.299999999999994, Iterations: 371940, Epsilon: 9.999999979125668e-05, Memory Size: 23886\n",
            "Episode: 5828, Total Reward: 3.799999999999999, Iterations: 371989, Epsilon: 9.999999979125668e-05, Memory Size: 23935\n",
            "Update Target Net\n",
            "Episode: 5829, Total Reward: 5.999999999999995, Iterations: 372051, Epsilon: 9.999999979125668e-05, Memory Size: 23997\n",
            "Episode: 5830, Total Reward: 3.799999999999999, Iterations: 372100, Epsilon: 9.999999979125668e-05, Memory Size: 24046\n",
            "Update Target Net\n",
            "Episode: 5831, Total Reward: 8.299999999999986, Iterations: 372185, Epsilon: 9.999999979125668e-05, Memory Size: 24131\n",
            "Update Target Net\n",
            "Episode: 5832, Total Reward: 8.999999999999984, Iterations: 372277, Epsilon: 9.999999979125668e-05, Memory Size: 24223\n",
            "Update Target Net\n",
            "Episode: 5833, Total Reward: 4.799999999999995, Iterations: 372336, Epsilon: 9.999999979125668e-05, Memory Size: 24282\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5834, Total Reward: 27.70000000000012, Iterations: 372579, Epsilon: 9.999999979125668e-05, Memory Size: 24525\n",
            "Update Target Net\n",
            "Episode: 5835, Total Reward: 3.799999999999999, Iterations: 372628, Epsilon: 9.999999979125668e-05, Memory Size: 24574\n",
            "Update Target Net\n",
            "Episode: 5836, Total Reward: 8.399999999999986, Iterations: 372714, Epsilon: 9.999999979125668e-05, Memory Size: 24660\n",
            "Update Target Net\n",
            "Episode: 5837, Total Reward: 10.799999999999981, Iterations: 372815, Epsilon: 9.999999979125668e-05, Memory Size: 24761\n",
            "Episode: 5838, Total Reward: 6.199999999999994, Iterations: 372879, Epsilon: 9.999999979125668e-05, Memory Size: 24825\n",
            "Update Target Net\n",
            "Episode: 5839, Total Reward: 3.9999999999999982, Iterations: 372930, Epsilon: 9.999999979125668e-05, Memory Size: 24876\n",
            "Episode: 5840, Total Reward: 4.399999999999997, Iterations: 372985, Epsilon: 9.999999979125668e-05, Memory Size: 24931\n",
            "Update Target Net\n",
            "Episode: 5841, Total Reward: 8.799999999999985, Iterations: 373075, Epsilon: 9.999999979125668e-05, Memory Size: 25021\n",
            "Update Target Net\n",
            "Episode: 5842, Total Reward: 3.799999999999999, Iterations: 373124, Epsilon: 9.999999979125668e-05, Memory Size: 25070\n",
            "Episode: 5843, Total Reward: 6.399999999999993, Iterations: 373190, Epsilon: 9.999999979125668e-05, Memory Size: 25136\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5844, Total Reward: 18.60000000000002, Iterations: 373360, Epsilon: 9.999999979125668e-05, Memory Size: 25306\n",
            "Update Target Net\n",
            "Episode: 5845, Total Reward: 8.299999999999986, Iterations: 373445, Epsilon: 9.999999979125668e-05, Memory Size: 25391\n",
            "Episode: 5846, Total Reward: 4.4999999999999964, Iterations: 373501, Epsilon: 9.999999979125668e-05, Memory Size: 25447\n",
            "Update Target Net\n",
            "Episode: 5847, Total Reward: 3.8999999999999986, Iterations: 373551, Epsilon: 9.999999979125668e-05, Memory Size: 25497\n",
            "Update Target Net\n",
            "Episode: 5848, Total Reward: 4.899999999999995, Iterations: 373611, Epsilon: 9.999999979125668e-05, Memory Size: 25557\n",
            "Episode: 5849, Total Reward: 4.799999999999995, Iterations: 373670, Epsilon: 9.999999979125668e-05, Memory Size: 25616\n",
            "Update Target Net\n",
            "Episode: 5850, Total Reward: 4.899999999999995, Iterations: 373730, Epsilon: 9.999999979125668e-05, Memory Size: 25676\n",
            "Episode: 5851, Total Reward: 4.699999999999996, Iterations: 373788, Epsilon: 9.999999979125668e-05, Memory Size: 25734\n",
            "Update Target Net\n",
            "Episode: 5852, Total Reward: 11.19999999999998, Iterations: 373893, Epsilon: 9.999999979125668e-05, Memory Size: 25839\n",
            "Update Target Net\n",
            "Episode: 5853, Total Reward: 3.799999999999999, Iterations: 373942, Epsilon: 9.999999979125668e-05, Memory Size: 25888\n",
            "Update Target Net\n",
            "Episode: 5854, Total Reward: 9.299999999999983, Iterations: 374037, Epsilon: 9.999999979125668e-05, Memory Size: 25983\n",
            "Episode: 5855, Total Reward: 3.799999999999999, Iterations: 374086, Epsilon: 9.999999979125668e-05, Memory Size: 26032\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5856, Total Reward: 13.99999999999997, Iterations: 374219, Epsilon: 9.999999979125668e-05, Memory Size: 26165\n",
            "Episode: 5857, Total Reward: 5.899999999999995, Iterations: 374280, Epsilon: 9.999999979125668e-05, Memory Size: 26226\n",
            "Update Target Net\n",
            "Episode: 5858, Total Reward: 5.999999999999995, Iterations: 374342, Epsilon: 9.999999979125668e-05, Memory Size: 26288\n",
            "Update Target Net\n",
            "Episode: 5859, Total Reward: 13.99999999999997, Iterations: 374475, Epsilon: 9.999999979125668e-05, Memory Size: 26421\n",
            "Update Target Net\n",
            "Episode: 5860, Total Reward: 5.999999999999995, Iterations: 374537, Epsilon: 9.999999979125668e-05, Memory Size: 26483\n",
            "Update Target Net\n",
            "Episode: 5861, Total Reward: 13.79999999999997, Iterations: 374668, Epsilon: 9.999999979125668e-05, Memory Size: 26614\n",
            "Update Target Net\n",
            "Episode: 5862, Total Reward: 4.899999999999995, Iterations: 374728, Epsilon: 9.999999979125668e-05, Memory Size: 26674\n",
            "Episode: 5863, Total Reward: 4.699999999999996, Iterations: 374786, Epsilon: 9.999999979125668e-05, Memory Size: 26732\n",
            "Update Target Net\n",
            "Episode: 5864, Total Reward: 8.899999999999984, Iterations: 374877, Epsilon: 9.999999979125668e-05, Memory Size: 26823\n",
            "Update Target Net\n",
            "Episode: 5865, Total Reward: 10.399999999999983, Iterations: 374974, Epsilon: 9.999999979125668e-05, Memory Size: 26920\n",
            "Update Target Net\n",
            "Episode: 5866, Total Reward: 8.299999999999986, Iterations: 375059, Epsilon: 9.999999979125668e-05, Memory Size: 27005\n",
            "Update Target Net\n",
            "Episode: 5867, Total Reward: 8.999999999999984, Iterations: 375151, Epsilon: 9.999999979125668e-05, Memory Size: 27097\n",
            "Update Target Net\n",
            "Episode: 5868, Total Reward: 4.799999999999995, Iterations: 375210, Epsilon: 9.999999979125668e-05, Memory Size: 27156\n",
            "Episode: 5869, Total Reward: 4.399999999999997, Iterations: 375265, Epsilon: 9.999999979125668e-05, Memory Size: 27211\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5870, Total Reward: 12.899999999999974, Iterations: 375387, Epsilon: 9.999999979125668e-05, Memory Size: 27333\n",
            "Episode: 5871, Total Reward: 4.599999999999996, Iterations: 375444, Epsilon: 9.999999979125668e-05, Memory Size: 27390\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5872, Total Reward: 19.800000000000022, Iterations: 375617, Epsilon: 9.999999979125668e-05, Memory Size: 27563\n",
            "Update Target Net\n",
            "Episode: 5873, Total Reward: 10.599999999999982, Iterations: 375716, Epsilon: 9.999999979125668e-05, Memory Size: 27662\n",
            "Episode: 5874, Total Reward: 3.8999999999999986, Iterations: 375766, Epsilon: 9.999999979125668e-05, Memory Size: 27712\n",
            "Update Target Net\n",
            "Episode: 5875, Total Reward: 3.799999999999999, Iterations: 375815, Epsilon: 9.999999979125668e-05, Memory Size: 27761\n",
            "Episode: 5876, Total Reward: 6.299999999999994, Iterations: 375880, Epsilon: 9.999999979125668e-05, Memory Size: 27826\n",
            "Update Target Net\n",
            "Episode: 5877, Total Reward: 4.899999999999995, Iterations: 375940, Epsilon: 9.999999979125668e-05, Memory Size: 27886\n",
            "Update Target Net\n",
            "Episode: 5878, Total Reward: 6.199999999999994, Iterations: 376004, Epsilon: 9.999999979125668e-05, Memory Size: 27950\n",
            "Update Target Net\n",
            "Episode: 5879, Total Reward: 9.099999999999984, Iterations: 376097, Epsilon: 9.999999979125668e-05, Memory Size: 28043\n",
            "Episode: 5880, Total Reward: 4.4999999999999964, Iterations: 376153, Epsilon: 9.999999979125668e-05, Memory Size: 28099\n",
            "Update Target Net\n",
            "Episode: 5881, Total Reward: 3.8999999999999986, Iterations: 376203, Epsilon: 9.999999979125668e-05, Memory Size: 28149\n",
            "Episode: 5882, Total Reward: 4.899999999999995, Iterations: 376263, Epsilon: 9.999999979125668e-05, Memory Size: 28209\n",
            "Update Target Net\n",
            "Episode: 5883, Total Reward: 8.299999999999986, Iterations: 376348, Epsilon: 9.999999979125668e-05, Memory Size: 28294\n",
            "Update Target Net\n",
            "Episode: 5884, Total Reward: 3.799999999999999, Iterations: 376397, Epsilon: 9.999999979125668e-05, Memory Size: 28343\n",
            "Episode: 5885, Total Reward: 3.8999999999999986, Iterations: 376447, Epsilon: 9.999999979125668e-05, Memory Size: 28393\n",
            "Update Target Net\n",
            "Episode: 5886, Total Reward: 3.799999999999999, Iterations: 376496, Epsilon: 9.999999979125668e-05, Memory Size: 28442\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5887, Total Reward: 20.200000000000028, Iterations: 376673, Epsilon: 9.999999979125668e-05, Memory Size: 28619\n",
            "Episode: 5888, Total Reward: 4.899999999999995, Iterations: 376733, Epsilon: 9.999999979125668e-05, Memory Size: 28679\n",
            "Update Target Net\n",
            "Episode: 5889, Total Reward: 13.399999999999972, Iterations: 376860, Epsilon: 9.999999979125668e-05, Memory Size: 28806\n",
            "Update Target Net\n",
            "Episode: 5890, Total Reward: 3.799999999999999, Iterations: 376909, Epsilon: 9.999999979125668e-05, Memory Size: 28855\n",
            "Update Target Net\n",
            "Episode: 5891, Total Reward: 17.500000000000004, Iterations: 377068, Epsilon: 9.999999979125668e-05, Memory Size: 29014\n",
            "Update Target Net\n",
            "Episode: 5892, Total Reward: 5.999999999999995, Iterations: 377130, Epsilon: 9.999999979125668e-05, Memory Size: 29076\n",
            "Update Target Net\n",
            "Episode: 5893, Total Reward: 13.199999999999973, Iterations: 377255, Epsilon: 9.999999979125668e-05, Memory Size: 29201\n",
            "Update Target Net\n",
            "Episode: 5894, Total Reward: 4.599999999999996, Iterations: 377312, Epsilon: 9.999999979125668e-05, Memory Size: 29258\n",
            "Episode: 5895, Total Reward: 3.8999999999999986, Iterations: 377362, Epsilon: 9.999999979125668e-05, Memory Size: 29308\n",
            "Update Target Net\n",
            "Episode: 5896, Total Reward: 8.299999999999986, Iterations: 377447, Epsilon: 9.999999979125668e-05, Memory Size: 29393\n",
            "Update Target Net\n",
            "Episode: 5897, Total Reward: 5.999999999999995, Iterations: 377509, Epsilon: 9.999999979125668e-05, Memory Size: 29455\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5898, Total Reward: 17.400000000000002, Iterations: 377667, Epsilon: 9.999999979125668e-05, Memory Size: 29613\n",
            "Episode: 5899, Total Reward: 3.799999999999999, Iterations: 377716, Epsilon: 9.999999979125668e-05, Memory Size: 29662\n",
            "Update Target Net\n",
            "Episode: 5900, Total Reward: 4.599999999999996, Iterations: 377773, Epsilon: 9.999999979125668e-05, Memory Size: 29719\n",
            "Episode: 5901, Total Reward: 8.399999999999986, Iterations: 377859, Epsilon: 9.999999979125668e-05, Memory Size: 29805\n",
            "Update Target Net\n",
            "Episode: 5902, Total Reward: 8.299999999999986, Iterations: 377944, Epsilon: 9.999999979125668e-05, Memory Size: 29890\n",
            "Update Target Net\n",
            "Episode: 5903, Total Reward: 6.299999999999994, Iterations: 378009, Epsilon: 9.999999979125668e-05, Memory Size: 29955\n",
            "Update Target Net\n",
            "Episode: 5904, Total Reward: 5.999999999999995, Iterations: 378071, Epsilon: 9.999999979125668e-05, Memory Size: 30017\n",
            "Update Target Net\n",
            "Episode: 5905, Total Reward: 10.399999999999983, Iterations: 378168, Epsilon: 9.999999979125668e-05, Memory Size: 30114\n",
            "Episode: 5906, Total Reward: 6.399999999999993, Iterations: 378234, Epsilon: 9.999999979125668e-05, Memory Size: 30180\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5907, Total Reward: 13.199999999999973, Iterations: 378359, Epsilon: 9.999999979125668e-05, Memory Size: 30305\n",
            "Episode: 5908, Total Reward: 6.399999999999993, Iterations: 378425, Epsilon: 9.999999979125668e-05, Memory Size: 30371\n",
            "Update Target Net\n",
            "Episode: 5909, Total Reward: 3.799999999999999, Iterations: 378474, Epsilon: 9.999999979125668e-05, Memory Size: 30420\n",
            "Update Target Net\n",
            "Episode: 5910, Total Reward: 8.399999999999986, Iterations: 378560, Epsilon: 9.999999979125668e-05, Memory Size: 30506\n",
            "Episode: 5911, Total Reward: 3.799999999999999, Iterations: 378609, Epsilon: 9.999999979125668e-05, Memory Size: 30555\n",
            "Update Target Net\n",
            "Episode: 5912, Total Reward: 6.299999999999994, Iterations: 378674, Epsilon: 9.999999979125668e-05, Memory Size: 30620\n",
            "Episode: 5913, Total Reward: 4.799999999999995, Iterations: 378733, Epsilon: 9.999999979125668e-05, Memory Size: 30679\n",
            "Update Target Net\n",
            "Episode: 5914, Total Reward: 6.399999999999993, Iterations: 378799, Epsilon: 9.999999979125668e-05, Memory Size: 30745\n",
            "Update Target Net\n",
            "Episode: 5915, Total Reward: 4.899999999999995, Iterations: 378859, Epsilon: 9.999999979125668e-05, Memory Size: 30805\n",
            "Episode: 5916, Total Reward: 8.499999999999986, Iterations: 378946, Epsilon: 9.999999979125668e-05, Memory Size: 30892\n",
            "Update Target Net\n",
            "Episode: 5917, Total Reward: 4.899999999999995, Iterations: 379006, Epsilon: 9.999999979125668e-05, Memory Size: 30952\n",
            "Update Target Net\n",
            "Episode: 5918, Total Reward: 13.099999999999973, Iterations: 379130, Epsilon: 9.999999979125668e-05, Memory Size: 31076\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5919, Total Reward: 14.09999999999997, Iterations: 379264, Epsilon: 9.999999979125668e-05, Memory Size: 31210\n",
            "Episode: 5920, Total Reward: 6.099999999999994, Iterations: 379327, Epsilon: 9.999999979125668e-05, Memory Size: 31273\n",
            "Update Target Net\n",
            "Episode: 5921, Total Reward: 4.099999999999998, Iterations: 379379, Epsilon: 9.999999979125668e-05, Memory Size: 31325\n",
            "Episode: 5922, Total Reward: 6.299999999999994, Iterations: 379444, Epsilon: 9.999999979125668e-05, Memory Size: 31390\n",
            "Update Target Net\n",
            "Episode: 5923, Total Reward: 3.799999999999999, Iterations: 379493, Epsilon: 9.999999979125668e-05, Memory Size: 31439\n",
            "Episode: 5924, Total Reward: 3.799999999999999, Iterations: 379542, Epsilon: 9.999999979125668e-05, Memory Size: 31488\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5925, Total Reward: 24.400000000000073, Iterations: 379752, Epsilon: 9.999999979125668e-05, Memory Size: 31698\n",
            "Episode: 5926, Total Reward: 6.099999999999994, Iterations: 379815, Epsilon: 9.999999979125668e-05, Memory Size: 31761\n",
            "Update Target Net\n",
            "Episode: 5927, Total Reward: 3.799999999999999, Iterations: 379864, Epsilon: 9.999999979125668e-05, Memory Size: 31810\n",
            "Update Target Net\n",
            "Episode: 5928, Total Reward: 10.799999999999981, Iterations: 379965, Epsilon: 9.999999979125668e-05, Memory Size: 31911\n",
            "Episode: 5929, Total Reward: 3.799999999999999, Iterations: 380014, Epsilon: 9.999999979125668e-05, Memory Size: 31960\n",
            "Update Target Net\n",
            "Episode: 5930, Total Reward: 4.599999999999996, Iterations: 380071, Epsilon: 9.999999979125668e-05, Memory Size: 32017\n",
            "Episode: 5931, Total Reward: 3.799999999999999, Iterations: 380120, Epsilon: 9.999999979125668e-05, Memory Size: 32066\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5932, Total Reward: 17.600000000000005, Iterations: 380280, Epsilon: 9.999999979125668e-05, Memory Size: 32226\n",
            "Episode: 5933, Total Reward: 3.799999999999999, Iterations: 380329, Epsilon: 9.999999979125668e-05, Memory Size: 32275\n",
            "Update Target Net\n",
            "Episode: 5934, Total Reward: 8.299999999999986, Iterations: 380414, Epsilon: 9.999999979125668e-05, Memory Size: 32360\n",
            "Update Target Net\n",
            "Episode: 5935, Total Reward: 5.899999999999995, Iterations: 380475, Epsilon: 9.999999979125668e-05, Memory Size: 32421\n",
            "Update Target Net\n",
            "Episode: 5936, Total Reward: 5.899999999999995, Iterations: 380536, Epsilon: 9.999999979125668e-05, Memory Size: 32482\n",
            "Episode: 5937, Total Reward: 4.799999999999995, Iterations: 380595, Epsilon: 9.999999979125668e-05, Memory Size: 32541\n",
            "Update Target Net\n",
            "Episode: 5938, Total Reward: 13.199999999999973, Iterations: 380720, Epsilon: 9.999999979125668e-05, Memory Size: 32666\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5939, Total Reward: 13.599999999999971, Iterations: 380849, Epsilon: 9.999999979125668e-05, Memory Size: 32795\n",
            "Update Target Net\n",
            "Episode: 5940, Total Reward: 20.200000000000028, Iterations: 381026, Epsilon: 9.999999979125668e-05, Memory Size: 32972\n",
            "Update Target Net\n",
            "Episode: 5941, Total Reward: 9.099999999999984, Iterations: 381119, Epsilon: 9.999999979125668e-05, Memory Size: 33065\n",
            "Update Target Net\n",
            "Episode: 5942, Total Reward: 10.799999999999981, Iterations: 381220, Epsilon: 9.999999979125668e-05, Memory Size: 33166\n",
            "Update Target Net\n",
            "Episode: 5943, Total Reward: 6.199999999999994, Iterations: 381284, Epsilon: 9.999999979125668e-05, Memory Size: 33230\n",
            "Update Target Net\n",
            "Episode: 5944, Total Reward: 3.799999999999999, Iterations: 381333, Epsilon: 9.999999979125668e-05, Memory Size: 33279\n",
            "Episode: 5945, Total Reward: 3.799999999999999, Iterations: 381382, Epsilon: 9.999999979125668e-05, Memory Size: 33328\n",
            "Update Target Net\n",
            "Episode: 5946, Total Reward: 3.799999999999999, Iterations: 381431, Epsilon: 9.999999979125668e-05, Memory Size: 33377\n",
            "Update Target Net\n",
            "Episode: 5947, Total Reward: 12.999999999999973, Iterations: 381554, Epsilon: 9.999999979125668e-05, Memory Size: 33500\n",
            "Episode: 5948, Total Reward: 4.899999999999995, Iterations: 381614, Epsilon: 9.999999979125668e-05, Memory Size: 33560\n",
            "Update Target Net\n",
            "Episode: 5949, Total Reward: 8.399999999999986, Iterations: 381700, Epsilon: 9.999999979125668e-05, Memory Size: 33646\n",
            "Update Target Net\n",
            "Episode: 5950, Total Reward: 6.299999999999994, Iterations: 381765, Epsilon: 9.999999979125668e-05, Memory Size: 33711\n",
            "Update Target Net\n",
            "Episode: 5951, Total Reward: 13.099999999999973, Iterations: 381889, Epsilon: 9.999999979125668e-05, Memory Size: 33835\n",
            "Update Target Net\n",
            "Episode: 5952, Total Reward: 3.799999999999999, Iterations: 381938, Epsilon: 9.999999979125668e-05, Memory Size: 33884\n",
            "Update Target Net\n",
            "Episode: 5953, Total Reward: 8.499999999999986, Iterations: 382025, Epsilon: 9.999999979125668e-05, Memory Size: 33971\n",
            "Update Target Net\n",
            "Episode: 5954, Total Reward: 10.399999999999983, Iterations: 382122, Epsilon: 9.999999979125668e-05, Memory Size: 34068\n",
            "Update Target Net\n",
            "Episode: 5955, Total Reward: 12.999999999999973, Iterations: 382245, Epsilon: 9.999999979125668e-05, Memory Size: 34191\n",
            "Update Target Net\n",
            "Episode: 5956, Total Reward: 9.299999999999983, Iterations: 382340, Epsilon: 9.999999979125668e-05, Memory Size: 34286\n",
            "Episode: 5957, Total Reward: 4.799999999999995, Iterations: 382399, Epsilon: 9.999999979125668e-05, Memory Size: 34345\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5958, Total Reward: 15.599999999999977, Iterations: 382539, Epsilon: 9.999999979125668e-05, Memory Size: 34485\n",
            "Episode: 5959, Total Reward: 4.599999999999996, Iterations: 382596, Epsilon: 9.999999979125668e-05, Memory Size: 34542\n",
            "Update Target Net\n",
            "Episode: 5960, Total Reward: 4.4999999999999964, Iterations: 382652, Epsilon: 9.999999979125668e-05, Memory Size: 34598\n",
            "Episode: 5961, Total Reward: 4.699999999999996, Iterations: 382710, Epsilon: 9.999999979125668e-05, Memory Size: 34656\n",
            "Update Target Net\n",
            "Episode: 5962, Total Reward: 3.799999999999999, Iterations: 382759, Epsilon: 9.999999979125668e-05, Memory Size: 34705\n",
            "Episode: 5963, Total Reward: 3.799999999999999, Iterations: 382808, Epsilon: 9.999999979125668e-05, Memory Size: 34754\n",
            "Update Target Net\n",
            "Episode: 5964, Total Reward: 9.099999999999984, Iterations: 382901, Epsilon: 9.999999979125668e-05, Memory Size: 34847\n",
            "Update Target Net\n",
            "Episode: 5965, Total Reward: 5.999999999999995, Iterations: 382963, Epsilon: 9.999999979125668e-05, Memory Size: 34909\n",
            "Update Target Net\n",
            "Episode: 5966, Total Reward: 3.799999999999999, Iterations: 383012, Epsilon: 9.999999979125668e-05, Memory Size: 34958\n",
            "Episode: 5967, Total Reward: 8.499999999999986, Iterations: 383099, Epsilon: 9.999999979125668e-05, Memory Size: 35045\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 5968, Total Reward: 22.200000000000056, Iterations: 383296, Epsilon: 9.999999979125668e-05, Memory Size: 35242\n",
            "Update Target Net\n",
            "Episode: 5969, Total Reward: 8.499999999999986, Iterations: 383383, Epsilon: 9.999999979125668e-05, Memory Size: 35329\n",
            "Update Target Net\n",
            "Episode: 5970, Total Reward: 10.399999999999983, Iterations: 383480, Epsilon: 9.999999979125668e-05, Memory Size: 35426\n",
            "Update Target Net\n",
            "Episode: 5971, Total Reward: 3.799999999999999, Iterations: 383529, Epsilon: 9.999999979125668e-05, Memory Size: 35475\n",
            "Update Target Net\n",
            "Episode: 5972, Total Reward: 13.399999999999972, Iterations: 383656, Epsilon: 9.999999979125668e-05, Memory Size: 35602\n",
            "Update Target Net\n",
            "Episode: 5973, Total Reward: 3.799999999999999, Iterations: 383705, Epsilon: 9.999999979125668e-05, Memory Size: 35651\n",
            "Episode: 5974, Total Reward: 5.999999999999995, Iterations: 383767, Epsilon: 9.999999979125668e-05, Memory Size: 35713\n",
            "Update Target Net\n",
            "Episode: 5975, Total Reward: 5.999999999999995, Iterations: 383829, Epsilon: 9.999999979125668e-05, Memory Size: 35775\n",
            "Episode: 5976, Total Reward: 5.999999999999995, Iterations: 383891, Epsilon: 9.999999979125668e-05, Memory Size: 35837\n",
            "Update Target Net\n",
            "Episode: 5977, Total Reward: 6.199999999999994, Iterations: 383955, Epsilon: 9.999999979125668e-05, Memory Size: 35901\n",
            "Update Target Net\n",
            "Episode: 5978, Total Reward: 12.999999999999973, Iterations: 384078, Epsilon: 9.999999979125668e-05, Memory Size: 36024\n",
            "Update Target Net\n",
            "Episode: 5979, Total Reward: 3.799999999999999, Iterations: 384127, Epsilon: 9.999999979125668e-05, Memory Size: 36073\n",
            "Episode: 5980, Total Reward: 6.299999999999994, Iterations: 384192, Epsilon: 9.999999979125668e-05, Memory Size: 36138\n",
            "Update Target Net\n",
            "Episode: 5981, Total Reward: 3.799999999999999, Iterations: 384241, Epsilon: 9.999999979125668e-05, Memory Size: 36187\n",
            "Update Target Net\n",
            "Episode: 5982, Total Reward: 5.999999999999995, Iterations: 384303, Epsilon: 9.999999979125668e-05, Memory Size: 36249\n",
            "Update Target Net\n",
            "Episode: 5983, Total Reward: 13.099999999999973, Iterations: 384427, Epsilon: 9.999999979125668e-05, Memory Size: 36373\n",
            "Update Target Net\n",
            "Episode: 5984, Total Reward: 11.09999999999998, Iterations: 384531, Epsilon: 9.999999979125668e-05, Memory Size: 36477\n",
            "Update Target Net\n",
            "Episode: 5985, Total Reward: 11.19999999999998, Iterations: 384636, Epsilon: 9.999999979125668e-05, Memory Size: 36582\n",
            "Update Target Net\n",
            "Episode: 5986, Total Reward: 6.299999999999994, Iterations: 384701, Epsilon: 9.999999979125668e-05, Memory Size: 36647\n",
            "Episode: 5987, Total Reward: 6.299999999999994, Iterations: 384766, Epsilon: 9.999999979125668e-05, Memory Size: 36712\n",
            "Update Target Net\n",
            "Episode: 5988, Total Reward: 3.799999999999999, Iterations: 384815, Epsilon: 9.999999979125668e-05, Memory Size: 36761\n",
            "Update Target Net\n",
            "Episode: 5989, Total Reward: 8.299999999999986, Iterations: 384900, Epsilon: 9.999999979125668e-05, Memory Size: 36846\n",
            "Update Target Net\n",
            "Episode: 5990, Total Reward: 10.799999999999981, Iterations: 385001, Epsilon: 9.999999979125668e-05, Memory Size: 36947\n",
            "Episode: 5991, Total Reward: 4.699999999999996, Iterations: 385059, Epsilon: 9.999999979125668e-05, Memory Size: 37005\n",
            "Update Target Net\n",
            "Episode: 5992, Total Reward: 4.899999999999995, Iterations: 385119, Epsilon: 9.999999979125668e-05, Memory Size: 37065\n",
            "Update Target Net\n",
            "Episode: 5993, Total Reward: 8.299999999999986, Iterations: 385204, Epsilon: 9.999999979125668e-05, Memory Size: 37150\n",
            "Episode: 5994, Total Reward: 3.799999999999999, Iterations: 385253, Epsilon: 9.999999979125668e-05, Memory Size: 37199\n",
            "Update Target Net\n",
            "Episode: 5995, Total Reward: 3.9999999999999982, Iterations: 385304, Epsilon: 9.999999979125668e-05, Memory Size: 37250\n",
            "Episode: 5996, Total Reward: 4.899999999999995, Iterations: 385364, Epsilon: 9.999999979125668e-05, Memory Size: 37310\n",
            "Update Target Net\n",
            "Episode: 5997, Total Reward: 4.699999999999996, Iterations: 385422, Epsilon: 9.999999979125668e-05, Memory Size: 37368\n",
            "Episode: 5998, Total Reward: 3.799999999999999, Iterations: 385471, Epsilon: 9.999999979125668e-05, Memory Size: 37417\n",
            "Update Target Net\n",
            "Episode: 5999, Total Reward: 9.399999999999983, Iterations: 385567, Epsilon: 9.999999979125668e-05, Memory Size: 37513\n",
            "Update Target Net\n",
            "Update Target Net\n",
            "Episode: 6000, Total Reward: 18.200000000000014, Iterations: 385733, Epsilon: 9.999999979125668e-05, Memory Size: 37679\n",
            "Update Target Net\n",
            "Episode: 6001, Total Reward: 13.299999999999972, Iterations: 385859, Epsilon: 9.999999979125668e-05, Memory Size: 37805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}